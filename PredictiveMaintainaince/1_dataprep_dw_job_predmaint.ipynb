{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored variables and their in-db values:\n",
      "dw_output_path_prm             -> 's3://ml-predictivemaintainaince-28-01-2022/export\n"
     ]
    }
   ],
   "source": [
    "%store -r\n",
    "%store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "import subprocess\n",
    "import sys\n",
    "orignal_version = pkg_resources.get_distribution(\"sagemaker\").version\n",
    "_ = subprocess.check_call([sys.executable,\"-m\",\"pip\",\"install\",\"sagemaker==2.20.0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import uuid\n",
    "import boto3\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters\n",
    "Configurable parameters used throughout the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s3 bucker for saving procesing job outputs\n",
    "sess = sagemaker.Session()\n",
    "bucket = \"ml-predictivemaintainaince-28-01-2022\"\n",
    "prefix = \"data_wrangler_flows\"\n",
    "flow_id = f\"{time.strftime('%d-%H-%M-%S',time.gmtime())}-{str(uuid.uuid4())[:8]}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_name = f\"flow-{flow_id}\"\n",
    "flow_uri = f\"s3://{bucket}/{prefix}/{flow_name}.flow\"\n",
    "flow_file_name = \"dw_flow/prm.flow\"\n",
    "iam_role = sagemaker.get_execution_role()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml-predictivemaintainaince-28-01-2022\n",
      "data_wrangler_flows\n",
      "29-13-00-36-29009274\n",
      "flow-29-13-00-36-29009274\n",
      "s3://ml-predictivemaintainaince-28-01-2022/data_wrangler_flows/flow-29-13-00-36-29009274.flow\n",
      "dw_flow/prm.flow\n",
      "arn:aws:iam::832173187970:role/service-role/AmazonSageMaker-ExecutionRole-20211213T210605\n"
     ]
    }
   ],
   "source": [
    "print(bucket)\n",
    "print(prefix)\n",
    "print(flow_id)\n",
    "print(flow_name)\n",
    "print(flow_uri)\n",
    "print(flow_file_name)\n",
    "print(iam_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Processing Job Resources Configurations\n",
    "# Data wrangler processing job only supports 1 instance.\n",
    "\n",
    "container_uri = (\n",
    "    \"415577184552.dkr.ecr.us-east-2.amazonaws.com/sagemaker-data-wrangler-container:1.2.1\"\n",
    ")\n",
    "instance_count = 1\n",
    "instance_type = \"ml.m5.4xlarge\"\n",
    "\n",
    "#Processing job path uri info\n",
    "\n",
    "output_prefix = f\"export-{flow_name}/output\"\n",
    "output_path = f\"s3://{bucket}/{output_prefix}\"\n",
    "output_name = \"ff586e7b-a02d-472b-91d4-da3dd05d7a30.default\"\n",
    "\n",
    "processing_job_name = f\"data-wrangler-flow-processing-{flow_id}\"\n",
    "processing_dir = \"/opt/ml/processing\"\n",
    "\n",
    "output_content_type = \"CSV\"\n",
    "\n",
    "sagemaker_endpoint_uri = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing dw_output_path_prm = s3://ml-predictivemaintainaince-28-01-2022/export-flow-29-13-00-36-29009274/output for use in next notebook 2_fleet_predmaint.ipynb\n",
      "Stored 'dw_output_path_prm' (str)\n"
     ]
    }
   ],
   "source": [
    "from demo_helper import update_dw_s3uri, get_dw_container_for_region\n",
    "\n",
    "\n",
    "# update the flow file to change the s3 location to our bucket\n",
    "#get the data wrangler container associated with our region\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "dw_output_path_prm = output_path\n",
    "print(f\"Storing dw_output_path_prm = {dw_output_path_prm} for use in next notebook 2_fleet_predmaint.ipynb\")\n",
    "%store dw_output_path_prm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ap-south-1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-7aadcf7d10a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcontainer_uri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dw_container_for_region\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/PredictiveMaintainaince/demo_helper.py\u001b[0m in \u001b[0;36mget_dw_container_for_region\u001b[0;34m(region_in)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mGet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mData\u001b[0m \u001b[0mWrangler\u001b[0m \u001b[0mcontainer\u001b[0m \u001b[0mbased\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mregion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \"\"\"\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mcontainer_uri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdw_container_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mregion_in\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontainer_uri\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ap-south-1'"
     ]
    }
   ],
   "source": [
    "container_uri = get_dw_container_for_region(region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load flowfile\n",
    "\n",
    "with open(flow_file_name) as f:\n",
    "    flow = json.load(f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Wrangler Flow notebook uploaded to s3://ml-predictivemaintainaince-28-01-2022/data_wrangler_flows/flow-29-13-00-36-29009274.flow\n"
     ]
    }
   ],
   "source": [
    "s3_client = boto3.client(\"s3\")\n",
    "s3_client.upload_file(flow_file_name,bucket,f\"{prefix}/{flow_name}.flow\")\n",
    "print(f\"Data Wrangler Flow notebook uploaded to {flow_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Processing Job Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.dataset_definition.inputs import (\n",
    "    AthenaDatasetDefinition,\n",
    "    DatasetDefinition,\n",
    "    RedshiftDatasetDefinition,\n",
    ")\n",
    "\n",
    "\n",
    "def create_flow_notebook_processing_input(base_dir, flow_s3_uri):\n",
    "    return ProcessingInput(\n",
    "        source=flow_s3_uri,\n",
    "        destination=f\"{base_dir}/flow\",\n",
    "        input_name=\"flow\",\n",
    "        s3_data_type=\"S3Prefix\",\n",
    "        s3_input_mode=\"File\",\n",
    "        s3_data_distribution_type=\"FullyReplicated\",\n",
    "    )\n",
    "\n",
    "\n",
    "def create_s3_processing_input(s3_dataset_definition, name, base_dir):\n",
    "    return ProcessingInput(\n",
    "        source=s3_dataset_definition[\"s3ExecutionContext\"][\"s3Uri\"],\n",
    "        destination=f\"{base_dir}/{name}\",\n",
    "        input_name=name,\n",
    "        s3_data_type=\"S3Prefix\",\n",
    "        s3_input_mode=\"File\",\n",
    "        s3_data_distribution_type=\"FullyReplicated\",\n",
    "    )\n",
    "\n",
    "\n",
    "def create_athena_processing_input(athena_dataset_defintion, name, base_dir):\n",
    "    return ProcessingInput(\n",
    "        input_name=name,\n",
    "        dataset_definition=DatasetDefinition(\n",
    "            local_path=f\"{base_dir}/{name}\",\n",
    "            athena_dataset_definition=AthenaDatasetDefinition(\n",
    "                catalog=athena_dataset_defintion[\"catalogName\"],\n",
    "                database=athena_dataset_defintion[\"databaseName\"],\n",
    "                query_string=athena_dataset_defintion[\"queryString\"],\n",
    "                output_s3_uri=athena_dataset_defintion[\"s3OutputLocation\"] + f\"{name}/\",\n",
    "                output_format=athena_dataset_defintion[\"outputFormat\"].upper(),\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def create_redshift_processing_input(redshift_dataset_defintion, name, base_dir):\n",
    "    return ProcessingInput(\n",
    "        input_name=name,\n",
    "        dataset_definition=DatasetDefinition(\n",
    "            local_path=f\"{base_dir}/{name}\",\n",
    "            redshift_dataset_definition=RedshiftDatasetDefinition(\n",
    "                cluster_id=redshift_dataset_defintion[\"clusterIdentifier\"],\n",
    "                database=redshift_dataset_defintion[\"database\"],\n",
    "                db_user=redshift_dataset_defintion[\"dbUser\"],\n",
    "                query_string=redshift_dataset_defintion[\"queryString\"],\n",
    "                cluster_role_arn=redshift_dataset_defintion[\"unloadIamRole\"],\n",
    "                output_s3_uri=redshift_dataset_defintion[\"s3OutputLocation\"] + f\"{name}/\",\n",
    "                output_format=redshift_dataset_defintion[\"outputFormat\"].upper(),\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def create_processing_inputs(processing_dir, flow, flow_uri):\n",
    "    \"\"\"Helper function for creating processing inputs\n",
    "    :param flow: loaded data wrangler flow notebook\n",
    "    :param flow_uri: S3 URI of the data wrangler flow notebook\n",
    "    \"\"\"\n",
    "    processing_inputs = []\n",
    "    flow_processing_input = create_flow_notebook_processing_input(processing_dir, flow_uri)\n",
    "    processing_inputs.append(flow_processing_input)\n",
    "\n",
    "    for node in flow[\"nodes\"]:\n",
    "        if \"dataset_definition\" in node[\"parameters\"]:\n",
    "            data_def = node[\"parameters\"][\"dataset_definition\"]\n",
    "            name = data_def[\"name\"]\n",
    "            source_type = data_def[\"datasetSourceType\"]\n",
    "\n",
    "            if source_type == \"S3\":\n",
    "                processing_inputs.append(create_s3_processing_input(data_def, name, processing_dir))\n",
    "            elif source_type == \"Athena\":\n",
    "                processing_inputs.append(\n",
    "                    create_athena_processing_input(data_def, name, processing_dir)\n",
    "                )\n",
    "            elif source_type == \"Redshift\":\n",
    "                processing_inputs.append(\n",
    "                    create_redshift_processing_input(data_def, name, processing_dir)\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"{source_type} is not supported for Data Wrangler Processing.\")\n",
    "\n",
    "    return processing_inputs\n",
    "\n",
    "\n",
    "def create_processing_output(output_name, output_path, processing_dir):\n",
    "    return ProcessingOutput(\n",
    "        output_name=output_name,\n",
    "        source=os.path.join(processing_dir, \"output\"),\n",
    "        destination=output_path,\n",
    "        s3_upload_mode=\"EndOfJob\",\n",
    "    )\n",
    "\n",
    "\n",
    "def create_container_arguments(output_name, output_content_type):\n",
    "    output_config = {output_name: {\"content_type\": output_content_type}}\n",
    "    return [f\"--output-config '{json.dumps(output_config)}'\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26 µs, sys: 0 ns, total: 26 µs\n",
      "Wall time: 29.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sagemaker.processing import Processor\n",
    "\n",
    "processor = Processor(\n",
    "    role = iam_role,\n",
    "    image_uri = container_uri,\n",
    "    instance_count = instance_count,\n",
    "    instance_type = instance_type,\n",
    "    sagemaker_session = sess\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  Predictive-Maintainaince-29-01-2022-18-50\n",
      "Inputs:  [{'InputName': 'flow', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://ml-predictivemaintainaince-28-01-2022/data_wrangler_flows/flow-29-13-00-36-29009274.flow', 'LocalPath': '/opt/ml/processing/flow', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'example_fleet_sensor_logs.csv', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://ml-predictivemaintainaince-28-01-2022/data_wrangler_flows/data/example_fleet_sensor_logs.csv', 'LocalPath': '/opt/ml/processing/example_fleet_sensor_logs.csv', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'example_fleet_info.csv', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://ml-predictivemaintainaince-28-01-2022/data_wrangler_flows/data/example_fleet_info.csv', 'LocalPath': '/opt/ml/processing/example_fleet_info.csv', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'ff586e7b-a02d-472b-91d4-da3dd05d7a30.default', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://ml-predictivemaintainaince-28-01-2022/export-flow-29-13-00-36-29009274/output', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ValidationException) when calling the CreateProcessingJob operation: Invalid region us-east-2 in image URI 415577184552.dkr.ecr.us-east-2.amazonaws.com/sagemaker-data-wrangler-container:1.2.1. Please provide an image URI in region ap-south-1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-0e959a7c1d70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mwait\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mjob_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Predictive-Maintainaince-29-01-2022-18-50\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/processing.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, inputs, outputs, arguments, wait, logs, job_name, experiment_config, kms_key)\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0marguments\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mstring\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mto\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0mprocessing\u001b[0m \u001b[0mjob\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m             \u001b[0minputs\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0msagemaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcessingInput\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mInput\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;32mfor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m                 \u001b[0mthe\u001b[0m \u001b[0mprocessing\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mThese\u001b[0m \u001b[0mmust\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mprovided\u001b[0m \u001b[0;32mas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m                 \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0msagemaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcessingInput\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobjects\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/processing.py\u001b[0m in \u001b[0;36mstart_new\u001b[0;34m(cls, processor, inputs, outputs, experiment_config)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mArgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m             \u001b[0mprocessing_job_name\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mName\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mprocessing\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    770\u001b[0m             \u001b[0msagemaker_session\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0msagemaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m                 \u001b[0mSession\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mwhich\u001b[0m \u001b[0mmanages\u001b[0m \u001b[0minteractions\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mAmazon\u001b[0m \u001b[0mSageMaker\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, inputs, output_config, job_name, resources, stopping_condition, app_specification, environment, network_config, role_arn, tags, experiment_config)\u001b[0m\n\u001b[1;32m    908\u001b[0m                 \u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m             \u001b[0mrole_arn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mAmazon\u001b[0m \u001b[0mResource\u001b[0m \u001b[0mName\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mARN\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mof\u001b[0m \u001b[0man\u001b[0m \u001b[0mIAM\u001b[0m \u001b[0mrole\u001b[0m \u001b[0mthat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m                 \u001b[0mAmazon\u001b[0m \u001b[0mSageMaker\u001b[0m \u001b[0mcan\u001b[0m \u001b[0massume\u001b[0m \u001b[0mto\u001b[0m \u001b[0mperform\u001b[0m \u001b[0mtasks\u001b[0m \u001b[0mon\u001b[0m \u001b[0myour\u001b[0m \u001b[0mbehalf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m             \u001b[0mtags\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mdictionaries\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m                 \u001b[0mpairs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    390\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ValidationException) when calling the CreateProcessingJob operation: Invalid region us-east-2 in image URI 415577184552.dkr.ecr.us-east-2.amazonaws.com/sagemaker-data-wrangler-container:1.2.1. Please provide an image URI in region ap-south-1."
     ]
    }
   ],
   "source": [
    "\n",
    "processor.run(\n",
    "    inputs=create_processing_inputs(processing_dir,flow,flow_uri),\n",
    "    outputs=[create_processing_output(output_name,output_path,processing_dir)],\n",
    "    arguments=create_container_arguments(output_name,output_content_type),\n",
    "    wait = True,\n",
    "    logs = False,\n",
    "    job_name = \"Predictive-Maintainaince-29-01-2022-18-50\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-south-1:394103062818:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
